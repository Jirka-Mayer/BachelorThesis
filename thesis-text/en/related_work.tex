\chapter*{Related work}
\addcontentsline{toc}{chapter}{Related work}

\section*{CVC-MUSCIMA dataset}
\addcontentsline{toc}{section}{CVC-MUSCIMA dataset}

\section*{MUSCIMA++ dataset}
\addcontentsline{toc}{section}{MUSCIMA++ dataset}

\section*{End-to-End OMR and the PrIMuS dataset}
\addcontentsline{toc}{section}{End-to-End OMR and the PrIMuS dataset}

\section*{HMR baseline article}
\addcontentsline{toc}{section}{HMR baseline article}

This section refers to the article: \cite{HmrBaseline}. This paper proposes a model that should serve as a baseline for handwritten music recognition. The model is again a convolutional recurrent neural network that recognises entire staves. The model is trained on printed music and then, using transfer learning, fine-tuned on handwritten music. The handwritten music comes from the MUSCIMA++ dataset and it has been varied using data augmentation (blurring, erosion, dilation, measure shuffling). This model, however, does not use CTC loss function, instead it produces two vectors for each pixel of the input image width. One vector contains symbols that are present in the image at that position and the other vector contains pitches of these symbols. This means annotations have to be aligned with the symbols (unlike with CTC), but it allows the model to recognise dense music sheets and even chords.

We will attempt to compare our model to the one from this article. The comparison will be difficult, because the output formats are so different, but we will mention all the differences and add a qualitative comparison of the final predictions. We want to utilize the fact that our evaluation dataset intersects with theirs and so we can perform direct comparison.
